{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f55a1868",
   "metadata": {},
   "source": [
    "Goal of this step1 is to ingest the documents and spilt them into chunks , embedded them and store them into a vector database eg (FAISS : Facebook AI Similarity Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4bec3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\sri19\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#Installing the lib and importing them\n",
    "\n",
    "!pip install -qU langchain cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bab876b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\sri19\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\sri19\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU \"faiss-cpu\" \"tiktoken\" \"PyPDF2\"\n",
    "!pip install  -qU \"sentence-transformers\" \"python-dotenv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde2eabc",
   "metadata": {},
   "source": [
    "Setting up API key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37d16cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb77820",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cryptography\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e514447",
   "metadata": {},
   "source": [
    "Loading the PDF / Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1868226b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 100 papers from arXiv JSON\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "documents = []\n",
    "\n",
    "# === 1. Load from arXiv JSON ===\n",
    "json_path = Path(\"../../archive/arxiv-metadata-oai-snapshot.json\")\n",
    "if json_path.exists():\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 100:  # Limit for testing; remove for full ingestion\n",
    "                break\n",
    "            entry = json.loads(line)\n",
    "\n",
    "            title = entry.get(\"title\", \"\").strip()\n",
    "            abstract = entry.get(\"abstract\", \"\").strip()\n",
    "            categories = entry.get(\"categories\", \"\").strip()\n",
    "            authors = entry.get(\"authors\", \"\").strip()\n",
    "            update_date = entry.get(\"update_date\", \"\")\n",
    "\n",
    "            content = f\"Title: {title}\\n\\nAbstract: {abstract}\\n\\nCategories: {categories}\"\n",
    "            metadata = {\n",
    "                \"arxiv_id\": entry.get(\"id\", \"\"),\n",
    "                \"authors\": authors,\n",
    "                \"update_date\": update_date\n",
    "            }\n",
    "\n",
    "            documents.append(Document(page_content=content, metadata=metadata))\n",
    "    print(f\" Loaded {len(documents)} papers from arXiv JSON\")\n",
    "else:\n",
    "    print(\" arXiv JSON file not found\")\n",
    "\n",
    "# === 2. Load new PDFs ===\n",
    "pdf_path = Path(\"../NewPapers\")\n",
    "if pdf_path.exists():\n",
    "    pdf_files = list(pdf_path.glob(\"*.pdf\"))\n",
    "    for file in pdf_files:\n",
    "        loader = PyPDFLoader(str(file))\n",
    "        pdf_docs = loader.load()\n",
    "        documents.extend(pdf_docs)\n",
    "    print(f\" Loaded {len(pdf_files)} PDFs with {len(documents)} total documents (JSON + PDF)\")\n",
    "else:\n",
    "    print(\" PDF folder ../NewPapers not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d369fbc",
   "metadata": {},
   "source": [
    "Now splitting the Document into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a91da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Small chunks for precise retrieval\n",
    "small_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "small_chunks = small_splitter.split_documents(documents)\n",
    "\n",
    "# Large chunks for broader context\n",
    "large_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "large_chunks = large_splitter.split_documents(documents)\n",
    "\n",
    "# In the above we need overlap so that we do not miss the context while using the chunks and I am using samll_chunks for facts and large_chunks for summerization task\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e0d5a",
   "metadata": {},
   "source": [
    "## Embedding Small chunks first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f497c4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded 387 small chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # progress bar\n",
    "import time\n",
    "\n",
    "def embed_in_batches(texts, batch_size=96):  # 96 is safe for Cohere's free tier\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        try:\n",
    "            response = co.embed(\n",
    "                texts=batch,\n",
    "                model=\"embed-english-v3.0\",\n",
    "                input_type=\"search_document\"\n",
    "            )\n",
    "            all_embeddings.extend(response.embeddings)\n",
    "        except Exception as e:\n",
    "            print(f\" Failed on batch {i // batch_size + 1}: {e}\")\n",
    "            time.sleep(5)  # Retry buffer for rate limits or server errors\n",
    "    return all_embeddings\n",
    "\n",
    "#  Run batching\n",
    "small_embeddings = embed_in_batches(small_texts)\n",
    "\n",
    "print(\"Embedded\", len(small_embeddings), \"small chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7678b327",
   "metadata": {},
   "source": [
    "## Embedding larger chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fb8e817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding large chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded 126 large chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def embed_in_batches(texts, batch_size=96, input_type=\"search_document\"):\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding large chunks\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        try:\n",
    "            response = co.embed(\n",
    "                texts=batch,\n",
    "                model=\"embed-english-v3.0\",\n",
    "                input_type=input_type\n",
    "            )\n",
    "            all_embeddings.extend(response.embeddings)\n",
    "        except Exception as e:\n",
    "            print(f\" Error on batch {i // batch_size + 1}: {e}\")\n",
    "            time.sleep(5)  # wait a bit before retrying\n",
    "    return all_embeddings\n",
    "\n",
    "#  Prepare texts\n",
    "large_texts = [chunk.page_content for chunk in large_chunks]\n",
    "\n",
    "# Embed with batching\n",
    "large_embeddings = embed_in_batches(large_texts)\n",
    "\n",
    "print(\"Embedded\", len(large_embeddings), \"large chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5431823",
   "metadata": {},
   "source": [
    "## Saving Small and Large Embeddings to FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da0fecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Convert small and large texts into LangChain documents\n",
    "small_docs = [Document(page_content=text) for text in small_texts]\n",
    "large_docs = [Document(page_content=text) for text in large_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "264c75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding wrapper\n",
    "\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "class WorkingCohereEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts):\n",
    "        response = co.embed(\n",
    "            texts=texts,\n",
    "            model=\"embed-english-v3.0\",\n",
    "            input_type=\"search_document\"\n",
    "        )\n",
    "        return response.embeddings\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "embedding_model = WorkingCohereEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de5575ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved small chunks to FAISS at vectorstore/faiss_small\n",
      " Saved large chunks to FAISS at vectorstore/faiss_large\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Save small chunk index\n",
    "faiss_small = FAISS.from_documents(small_docs, embedding_model)\n",
    "faiss_small.save_local(\"vectorstore/faiss_small\", index_name=\"small_index\")\n",
    "print(\"Saved small chunks to FAISS at vectorstore/faiss_small\")\n",
    "\n",
    "# Save large chunk index\n",
    "faiss_large = FAISS.from_documents(large_docs, embedding_model)\n",
    "faiss_large.save_local(\"vectorstore/faiss_large\", index_name=\"large_index\")\n",
    "print(\" Saved large chunks to FAISS at vectorstore/faiss_large\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9e6a79",
   "metadata": {},
   "source": [
    "## Load the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e078a79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Both FAISS indexes loaded.\n"
     ]
    }
   ],
   "source": [
    "# Small chunks for factual queries\n",
    "vector_store_small = FAISS.load_local(\n",
    "    folder_path=\"vectorstore/faiss_small\",\n",
    "    index_name=\"small_index\",\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# Large chunks for summary/reasoning queries\n",
    "vector_store_large = FAISS.load_local(\n",
    "    folder_path=\"vectorstore/faiss_large\",\n",
    "    index_name=\"large_index\",\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "print(\" Both FAISS indexes loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53e285a",
   "metadata": {},
   "source": [
    "## Retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7432f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use k=3 for top-3 most similar chunks\n",
    "retriever_small = vector_store_small.as_retriever(search_kwargs={\"k\": 3})\n",
    "retriever_large = vector_store_large.as_retriever(search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e0e497",
   "metadata": {},
   "source": [
    "## Creating RAG Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fbb576d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sri19\\AppData\\Local\\Temp\\ipykernel_32148\\1668169493.py:4: LangChainDeprecationWarning: The class `Cohere` was deprecated in LangChain 0.1.14 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import Cohere``.\n",
      "  llm = CohereLLM(cohere_api_key=api_key)\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Cohere as CohereLLM\n",
    "\n",
    "llm = CohereLLM(cohere_api_key=api_key)\n",
    "\n",
    "# Chain for factual Q&A\n",
    "rag_chain_small = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever_small,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Chain for summary-level reasoning\n",
    "rag_chain_large = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever_large,\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809b5f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How many papers do you have with you using the json file that I providedd ?\"\n",
    "result = rag_chain_small.invoke(query)\n",
    "print(\"ðŸ”¹ Answer:\\n\", result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6c50a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
