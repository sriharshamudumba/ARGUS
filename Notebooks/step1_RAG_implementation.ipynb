{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f55a1868",
   "metadata": {},
   "source": [
    "Goal of this step1 is to ingest the documents and spilt them into chunks , embedded them and store them into a vector database eg (FAISS : Facebook AI Similarity Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4bec3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\sri19\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#Installing the lib and importing them\n",
    "\n",
    "!pip install -qU langchain cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bab876b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\sri19\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\sri19\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU \"faiss-cpu\" \"tiktoken\" \"PyPDF2\"\n",
    "!pip install  -qU \"sentence-transformers\" \"python-dotenv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde2eabc",
   "metadata": {},
   "source": [
    "Setting up API key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37d16cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb77820",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cryptography\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e514447",
   "metadata": {},
   "source": [
    "Loading the PDF / Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1868226b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 100 documents from arXiv JSON\n",
      " Loaded 2 PDFs with 119 total documents\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "documents = []\n",
    "\n",
    "# === 1. Load from arXiv JSON ===\n",
    "json_path = Path(\"../../archive/arxiv-metadata-oai-snapshot.json\")\n",
    "if json_path.exists():\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 100:  \n",
    "                break\n",
    "            entry = json.loads(line)\n",
    "            content = f\"Title: {entry.get('title','').strip()}\\n\\nAbstract: {entry.get('abstract','').strip()}\\n\\nCategories: {entry.get('categories','').strip()}\"\n",
    "            metadata = {\n",
    "                \"arxiv_id\": entry.get(\"id\", \"\"),\n",
    "                \"authors\": entry.get(\"authors\", \"\"),\n",
    "                \"update_date\": entry.get(\"update_date\", \"\")\n",
    "            }\n",
    "            documents.append(Document(page_content=content, metadata=metadata))\n",
    "    print(f\" Loaded {len(documents)} documents from arXiv JSON\")\n",
    "else:\n",
    "    print(\" arXiv JSON not found\")\n",
    "\n",
    "# === 2. Load PDFs ===\n",
    "pdf_path = Path(\"../Papers\")\n",
    "if pdf_path.exists():\n",
    "    pdf_files = list(pdf_path.glob(\"*.pdf\"))\n",
    "    for file in pdf_files:\n",
    "        loader = PyPDFLoader(str(file))\n",
    "        pdf_docs = loader.load()\n",
    "        documents.extend(pdf_docs)\n",
    "    print(f\" Loaded {len(pdf_files)} PDFs with {len(documents)} total documents\")\n",
    "else:\n",
    "    print(\" PDF folder not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d369fbc",
   "metadata": {},
   "source": [
    "Now splitting the Document into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a91da77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 676 small chunks, 221 large chunks.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Small\n",
    "small_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "small_chunks = small_splitter.split_documents(documents)\n",
    "\n",
    "# Large\n",
    "large_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "large_chunks = large_splitter.split_documents(documents)\n",
    "\n",
    "# Prepare texts for embedding\n",
    "small_texts = [chunk.page_content for chunk in small_chunks]\n",
    "large_texts = [chunk.page_content for chunk in large_chunks]\n",
    "\n",
    "print(f\" {len(small_chunks)} small chunks, {len(large_chunks)} large chunks.\")\n",
    "\n",
    "\n",
    "# In the above we need overlap so that we do not miss the context while using the chunks and I am using samll_chunks for facts and large_chunks for summerization task\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e0d5a",
   "metadata": {},
   "source": [
    "## Embedding  chunks first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f497c4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:01<00:02,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Batch 4 failed: status_code: 502, body: \n",
      "<html><head>\n",
      "<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\n",
      "<title>502 Server Error</title>\n",
      "</head>\n",
      "<body text=#000000 bgcolor=#ffffff>\n",
      "<h1>Error: Server Error</h1>\n",
      "<h2>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds.</h2>\n",
      "<h2></h2>\n",
      "</body></html>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:32<00:00, 19.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded 580 small chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded 221 large chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def embed_in_batches(texts, batch_size=96, input_type=\"search_document\"):\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        try:\n",
    "            response = co.embed(\n",
    "                texts=batch,\n",
    "                model=\"embed-english-v3.0\",\n",
    "                input_type=input_type\n",
    "            )\n",
    "            all_embeddings.extend(response.embeddings)\n",
    "        except Exception as e:\n",
    "            print(f\" Batch {i // batch_size + 1} failed: {e}\")\n",
    "            time.sleep(5)\n",
    "    return all_embeddings\n",
    "\n",
    "# Run for small & large\n",
    "small_embeddings = embed_in_batches(small_texts)\n",
    "print(\"Embedded\", len(small_embeddings), \"small chunks\")\n",
    "\n",
    "large_embeddings = embed_in_batches(large_texts)\n",
    "print(\"Embedded\", len(large_embeddings), \"large chunks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5431823",
   "metadata": {},
   "source": [
    "## Saving Small and Large Embeddings to FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0fecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Wrap embeddings\n",
    "class WorkingCohereEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts):\n",
    "        return co.embed(texts=texts, model=\"embed-english-v3.0\", input_type=\"search_document\").embeddings\n",
    "    def embed_query(self, text):\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "embedding_model = WorkingCohereEmbeddings()\n",
    "\n",
    "# Convert texts to Documents\n",
    "small_docs = [Document(page_content=text) for text in small_texts]\n",
    "large_docs = [Document(page_content=text) for text in large_texts]\n",
    "\n",
    "# Save to FAISS\n",
    "faiss_small = FAISS.from_documents(small_docs, embedding_model)\n",
    "faiss_small.save_local(\"vectorstore/faiss_small\", index_name=\"small_index\")\n",
    "\n",
    "faiss_large = FAISS.from_documents(large_docs, embedding_model)\n",
    "faiss_large.save_local(\"vectorstore/faiss_large\", index_name=\"large_index\")\n",
    "\n",
    "print(\" Both small and large FAISS indexes saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "264c75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding wrapper\n",
    "\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "class WorkingCohereEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts):\n",
    "        response = co.embed(\n",
    "            texts=texts,\n",
    "            model=\"embed-english-v3.0\",\n",
    "            input_type=\"search_document\"\n",
    "        )\n",
    "        return response.embeddings\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "embedding_model = WorkingCohereEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5575ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Save small chunk index\n",
    "faiss_small = FAISS.from_documents(small_docs, embedding_model)\n",
    "faiss_small.save_local(\"vectorstore/faiss_small\", index_name=\"small_index\")\n",
    "print(\"Saved small chunks to FAISS at vectorstore/faiss_small\")\n",
    "\n",
    "# Save large chunk index\n",
    "faiss_large = FAISS.from_documents(large_docs, embedding_model)\n",
    "faiss_large.save_local(\"vectorstore/faiss_large\", index_name=\"large_index\")\n",
    "print(\" Saved large chunks to FAISS at vectorstore/faiss_large\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9e6a79",
   "metadata": {},
   "source": [
    "## Load the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e078a79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Both FAISS indexes loaded.\n"
     ]
    }
   ],
   "source": [
    "# Small chunks for factual queries\n",
    "vector_store_small = FAISS.load_local(\n",
    "    folder_path=\"vectorstore/faiss_small\",\n",
    "    index_name=\"small_index\",\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# Large chunks for summary/reasoning queries\n",
    "vector_store_large = FAISS.load_local(\n",
    "    folder_path=\"vectorstore/faiss_large\",\n",
    "    index_name=\"large_index\",\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "print(\" Both FAISS indexes loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53e285a",
   "metadata": {},
   "source": [
    "## Retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7432f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use k=3 for top-3 most similar chunks\n",
    "retriever_small = vector_store_small.as_retriever(search_kwargs={\"k\": 3})\n",
    "retriever_large = vector_store_large.as_retriever(search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e0e497",
   "metadata": {},
   "source": [
    "## Creating RAG Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0fbb576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import Cohere as CohereLLM\n",
    "\n",
    "llm = CohereLLM(cohere_api_key=api_key)\n",
    "\n",
    "# Chain for factual Q&A\n",
    "rag_chain_small = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever_small,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Chain for summary-level reasoning\n",
    "rag_chain_large = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever_large,\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "809b5f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Answer:\n",
      "  I don't have a specific category or topic named 'memory and performance', but I do have categories related to cognitive computing and machine learning, which may be of interest to you. These include: \n",
      "\n",
      "- quant-ph: This category focuses on quantum physics, including quantum information and computation, which may have implications for advanced computing and memory systems. \n",
      "\n",
      "- cs.IT and math.IT: These categories cover computational intelligence and mathematical modeling, which are often applied to developing innovative computing systems and optimizing performance. \n",
      "\n",
      "- cond-mat.stat-mech and cond-mat.mtrl-sci: These categories concern the study of matter and materials behavior, including statistical mechanics and condensed matter physics, which could involve research related to memory storage or performance. \n",
      "\n",
      "If you have a specific question related to memory or performance, please let me know, and I'll do my best to answer it or guide you to additional resources. \n"
     ]
    }
   ],
   "source": [
    "query = \"Do you have a categories for realted to memory and performance?\"\n",
    "result = rag_chain_small.invoke(query)\n",
    "print(\"ðŸ”¹ Answer:\\n\", result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6c50a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
